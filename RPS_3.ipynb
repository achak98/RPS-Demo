{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RPS_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jatinkarthik-tripathy/RPS-Demo/blob/master/RPS_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwkqRKY8JPZp",
        "colab_type": "code",
        "outputId": "bf97b482-2a34-40b6-8183-e8dee9ef35f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-mK69LkO4dM",
        "colab_type": "code",
        "outputId": "8c0ac709-edd1-405c-b03d-306d50558cd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agp3UJ79QO-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Flatten\n",
        "from tensorflow.keras.layers import Dropout, Concatenate, Bidirectional\n",
        "from tensorflow.keras.models import Model\n",
        "from keras import metrics\n",
        "\n",
        "\n",
        "class RPS_model:\n",
        "    def __init__(self, player_vocab_size, max_inp_len, max_out_len, embed_size=256):\n",
        "        self.final_model = Sequential()\n",
        "        self.final_model.add(Embedding(input_dim=player_vocab_size, \n",
        "                                       output_dim=embed_size,\n",
        "                                       input_length=max_inp_len))\n",
        "        self.final_model.add(LSTM(128, return_sequences=True))\n",
        "        self.final_model.add(LSTM(128, return_sequences=True))\n",
        "        self.final_model.add(LSTM(128))\n",
        "        self.final_model.add(Dense(128, activation='relu'))\n",
        "        self.final_model.add(Dropout(.2))\n",
        "        self.final_model.add(Dense(64, activation='relu'))\n",
        "        self.final_model.add(Dropout(.1))\n",
        "        self.final_model.add(Dense(32, activation='relu'))\n",
        "        self.final_model.add(Dense(8, activation='relu'))\n",
        "        self.final_model.add(Dense(max_out_len, activation='softmax'))\n",
        "        self.final_model.compile(loss=\"categorical_crossentropy\", \n",
        "                                 optimizer=\"adam\", \n",
        "                                 metrics=['accuracy'])\n",
        "        # print(self.final_model.summary())\n",
        "\n",
        "    def train(self, x_train, y_train, batch=100, eps=5, cbs=[]):\n",
        "        self.final_model.fit(x_train, y_train,\n",
        "                             batch_size=batch, epochs=eps, callbacks=cbs)\n",
        "\n",
        "    def save_model(self, model_name):\n",
        "        self.final_model.save(model_name)\n",
        "\n",
        "    def load_model(self, model_name):\n",
        "        self.final_model.load_weights(model_name)\n",
        "\n",
        "    def test(self, p1_test, p2_test, status_test, p1_next_test, batch=100):\n",
        "        score, accuracy = self.final_model.evaluate(\n",
        "            [p1_test, p2_test, status_test], p1_next_test, batch_size=batch)\n",
        "        print(f\"Test fraction correct (NN-Score) = {score}\")\n",
        "        print(f\"Test fraction correct (NN-Accuracy) = {accuracy}\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA1k2ZgD7sDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def to_num(x):\n",
        "#   if x == 'R':\n",
        "#     return 1.\n",
        "#   if x == 'P':\n",
        "#     return 2.    \n",
        "#   if x == 'S':\n",
        "#     return 3.\n",
        "\n",
        "# data_path = \"/content/drive/My Drive/Models/RPS_2/train_data.txt\"\n",
        "# with open(data_path, 'r', encoding='utf-8') as f:\n",
        "#     lines = f.read().split('\\n')\n",
        "\n",
        "# x_train = []\n",
        "# y_train = []\n",
        "# x_temp = []\n",
        "# for line in lines[:5_000]:\n",
        "#   _, p1, p2, _, p1n = line.split('\\t')\n",
        "#   if p1n != '_' and len(x_temp)<50:\n",
        "#     x_temp.append(to_num(p1))\n",
        "#     x_temp.append(to_num(p2))\n",
        "#     x_train.append([x for x in x_temp])\n",
        "#     y_train.append(to_num(p1n))\n",
        "#   else:\n",
        "#     x_temp = []\n",
        "\n",
        "# y_train = to_categorical(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mPvZKwf_8H-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = numpy.loadtxt(\"/content/drive/My Drive/Models/RPS_3/moves.csv\", delimiter=\",\")\n",
        "\n",
        "X = dataset[:,0:18]\n",
        "Y = dataset[:,18]\n",
        "\n",
        "Y=to_categorical(Y)\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Acs0ZIIYe57D",
        "colab_type": "code",
        "outputId": "2ec8174a-e944-47e2-ab6c-9db096a4eb69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "print(X[969])\n",
        "print(Y[969])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3. 2. 1. 3. 2. 2. 3. 2. 1. 3. 2. 2. 3. 2. 1. 1. 2. 2.]\n",
            "[0. 0. 0. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZZVqJJHNDF2",
        "colab_type": "code",
        "outputId": "2bc751f7-8adf-4256-abc1-dd872f6ba66c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "vocab = set()\n",
        "for i in [1, 2, 3]:\n",
        "  vocab.add(i)\n",
        "vocab = sorted(list(vocab))\n",
        "vocab_size = len(vocab) + 1\n",
        "max_inp_len = max([len(txt) for txt in x_train])\n",
        "max_out_len = max([len(txt) for txt in y_train])\n",
        "token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(vocab)])\n",
        "\n",
        "print(f'Number of samples: {len(x_train)}')\n",
        "print(f'Vocab size: {vocab_size}')\n",
        "print(f'Max input len: {max_inp_len}')\n",
        "print(f'Max output len: {max_out_len}')\n",
        "\n",
        "\n",
        "print(f'len train set: {len(x_train)}')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 1604\n",
            "Vocab size: 4\n",
            "Max input len: 18\n",
            "Max output len: 4\n",
            "len train set: 1604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3npMs7sV69p-",
        "colab_type": "code",
        "outputId": "cd349368-1220-404c-db75-7e8827313d31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "source": [
        "checkpoint_path = \"/content/drive/My Drive/Ckpts/RPS3_t2/cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create checkpoint callback\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=0)\n",
        "\n",
        "logdir = \"/content/logs/\"\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "reduce_lr=tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.9, \n",
        "                             verbose=1, mode='auto', cooldown=30, min_lr=0.0001)\n",
        "\n",
        "\n",
        "old_model_name = ''\n",
        "model_name = 'rps3_7.h5'\n",
        "model = RPS_model(vocab_size, max_inp_len, max_out_len)\n",
        "try:\n",
        "  model.load_model(f\"/content/drive/My Drive/Models/RPS_3/{old_model_name}\")\n",
        "  print(f'{old_model_name} loaded')\n",
        "except:\n",
        "  print('Model not trained yet')\n",
        "model.train( x_train, y_train, eps=250, batch=100, \n",
        "            cbs=[tensorboard_callback, cp_callback, reduce_lr])\n",
        "model.save_model(f\"/content/drive/My Drive/Models/RPS_3/{model_name}\")\n",
        "print(f\"{model_name} saved\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model not trained yet\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-c7c726ed5b4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model not trained yet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m model.train( x_train, y_train, eps=250, batch=100, \n\u001b[0;32m---> 25\u001b[0;31m             cbs=[tensorboard_callback, cp_callback, reduce_lr])\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"/content/drive/My Drive/Models/RPS_3/{model_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{model_name} saved\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-e71b8ff948bc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x_train, y_train, batch, eps, cbs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         self.final_model.fit(x_train, y_train,\n\u001b[0;32m---> 31\u001b[0;31m                              batch_size=batch, epochs=eps, callbacks=cbs)\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2535\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2536\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[0;32m-> 2537\u001b[0;31m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m   2538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2539\u001b[0m       \u001b[0;31m# If sample weight mode has not been set and weights are None for all the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    739\u001b[0m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[1;32m    740\u001b[0m                            \u001b[0;34m' was passed for an output of shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m                            \u001b[0;34m' while using as loss `'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m                            \u001b[0;34m'This loss expects targets to have the same shape '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                            'as the output.')\n",
            "\u001b[0;31mValueError\u001b[0m: A target array with shape (1604, 4) was passed for an output of shape (None, 18, 4) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48jeAWoTxLrE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.load_model(\"/content/drive/My Drive/Models/RPS_3/rps3_5.h5\")\n",
        "print(model.summary())\n",
        "print(f'Number of samples: {len(x_test)}')\n",
        "print(f'Vocab size: {vocab_size}')\n",
        "print(f'Max input len: {max_inp_len}')\n",
        "print(f'Max output len: {max_out_len}')\n",
        "\n",
        "\n",
        "# x_test = pad_sequences(x_test, maxlen=max_inp_len, padding='post')\n",
        "# x_test, y_test = shuffle(numpy.array(x_test, dtype='float32'), \n",
        "#                            numpy.array(y_test, dtype='float32'))\n",
        "\n",
        "results = model.evaluate(x_test, y_test)\n",
        "print(f'results: {results}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIDocPmqZxxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}